## AWS를 활용한 인공지능 모델 배포



#### Cloud computing 이전

- 과거에는 인터넷 환경에서 서비스를 제공하기 위해 서비스 제공자는 서비스 호스팅에 필요한 모든 것을 직접 구축
- 데이터 센터를 처음 구축할 때, 서비스 아키텍처나 자원 예상 사용량 등을 고려해 구축
- 하지만 서버를 직접 구축하고 운영하는 자원과 인력의 비용이 크고 운영 상황의 변화에 능동적으로 대응하기가 어려움
- IDC(Internet Data Center) 서버 운영에 필요한 공간, 네트워크, 유지 보수 등의 서비스를 제공함
  - 그러나 일정 기간 임대를 하는 방식으로 유연성이 떨어지는 구조



#### Backgrounds of Cloud computing

- 인터넷 사용자가 크게 증가하고 다양한 서비스를 제공하게 되면서 필요한 때에 필요한 만큼 서버를 증설하기 원하는 온디맨드 수요 증가



#### Cloud Computing

- 인터넷 기반 컴퓨팅의 일종
- 언제 어디서나 필요한 만큼의 컴퓨터 자원을 필요한 시간만큼 인터넷을 통하여 활용할 수 있는 컴퓨팅 방식
- 2006년 아마존이 클라우드를 통한 저장공간 및 연산 자원 제공 서비스인 S3와 EC2를 개시하면서 본격적인 클라우드 컴퓨팅 시대 시작
- AWS는 클라우드 컴퓨팅을 클라우드 서비스 플랫폼에서 컴퓨팅 파워, DB 저장공간, 애플리케이션 및 기타 IT 자원을 필요에 따라 인터넷을 통해 제공하고 사용한 만큼만 비용을 지불하는 것으로 정의

##### 4차 산업혁명 시대에서 빅데이터의 수집, 저장, 분석을 위한 방대한 컴퓨팅 자원과 인공지능 개발을 위한 고성능 컴퓨터를 스타트업이나 중소기업이 처음부터 모든 것을 별도로 구입하지 않고도 적은 비용으로 빠르게 필요한 IT 환경 마련 가능



#### Features of Cloud Computing

- 속도 - 주문형 셀프서비스
  - 제공자와 커뮤니케이션 없이 바로 이용 가능
- 접근성
  - 인터넷을 통해 사용자의 위치, 시간에 관계없이 접근 가능
- 확장성
  - 갑작스러운 이용량 증가나 변화에 신속 유연하게 추가 확장 가능
- 생산성
  - 하드웨어, 소프트웨어 설치에 들어가는 시간, 비용 절감으로 핵심 업무에 집중
- 보안, 안정성
  -  공급자가 전체적으로 보안이나 안정성에 대해 준비
- 측정 가능성
  - 분, 초 단위로 사용자가 클라우드 서비스를 사용한 만큼만 계량하여 과금



#### 클라우드 컴퓨팅 운용 모델

- 퍼블릭(Public)
  - 서비스 유지를 위한 모든 인프라와 IT기술을 클라우드에서 사용
  - IT관리 인력이나 인프라 구축 비용이 없는 경우 유용
- 프라이빗(Private)
  - 고객이 자체 데이터센터에서 직접 클라우드 서비스를 구축
  - 내부 계열사나 고객에게만 제공하여 인프라 확충은 쉬우나 IT기술확보가 어려움
  - 보안이 좋고 커스터마이제이션이 가능하며 글로벌 클라우드 사업자가 IT기술만 패키지형태로 판매하기도 함
- 하이브리드(Hybrid)
  - 고객의 핵심 시스템은 내부에 두면서도 외부의 클라우드를 활용하는 형태
  - IT기술은 클라우드에서 받고 서비스 유지를 위한 인프라는 고객의 것을 혼용
  - 퍼블릭의 경제성과 프라이빗의 보안성을 모두 고려



#### 클라우드 서비스 제공 모델

ex)피자의 예/ 자동차의 예

- On-premises
  - 직접 모든 재료를 사고 조리 및 상차리기까지 혼자 해결
  - 직접 차를 소유 및 운전
- IaaS
  - 재료 및 준비는 배달받고, 조리 및 상차리기 등은 혼자 해결
  - 자동차 리스 형태 - 운전은 직접
- PaaS
  - 피자를 배달 받고, 상차리기만 직접 해결
  - 택시를 타는것과 비슷 - 어느정도 자유 보장
- SaaS
  - 주문 제작부터 상차리기까지 제공 받음
  - 버스를 타는 것처럼 정해진 서비스를 제공받음

![image](https://user-images.githubusercontent.com/60081212/117775142-2b29a780-b275-11eb-8d46-bb09a4057256.png)





## 실습 - AWS 실습 환경 세팅

vscode - remote ssh current window~

ssh -i "C:\Users\User\Desktop\devschool\w4\cjw9156.pem" ubuntu@ec2-13-124-28-218.ap-northeast-2.compute.amazonaws.com





연결 완료



## API to serve ML model

AWS EC2와 Python Flask 기반 모델 학습 및 추론을 요청/응답하는 API 서버 개발

####  Interface

사용자는 기계와 소프트웨어를 제어하기 위해 인터페이스를 정해진 메뉴얼에 따라 활용하여 원하는 경험을 획득

- 인터페이스는 상호 합의된 매뉴얼에 따라 적절한 입력을 받아 기대되는 출력을 제공할 수 있어야 함.



#### API란?

Application Programming Interface의 약자로 기계와 기계, 소프트웨어와 소프트웨어 간의 커뮤니케이션을 위한 인터페이스를 의미



#### RESTful API for ML/DL model inference

REST 아키텍처를 따르는 API로 HTTP URI를 통해 자원을 명시하고 HTTP method를 통해 필요한 연산을 요청하고 반환하는 API를 지칭



#### Practical process of machine learning

- 문제정의,  데이터준비, 모델 학습 및 검증, 모델 배포, 모니터링 등의 과정을 통해 실제 서비스에 기계학습 모델을 적용



#### Model Serving

- 학습의 모델을 REST API 방식으로 배포하기 위해서 학습된 모델의 Serialization과 웹 프레임워크를 통해 배포 준비 필요

1. Model Training
2. Serializing Model
3. Serving Model

- 모델을 서빙할 때는 학습 시의 데이터 분포나 처리 방법과의 연속성 유지 필요
- 모델을 배포하는 환경에 따라 다양한 Serving Framework를 고려하여 활용



#### Serialization & De-serialization

- 학습한 모델의 재사용 및 배포를 위해서 저장하고 불러오는 것
- Serialization을 통해 ML/DL model object를 disk에 write하여 어디든 전송하고 불러올 수 있는 형태로 변환
- De-serialization을 통해 Python 혹은 다른 환경에서 model을 불러와 추론/학습에 사용
- 모델을 배포하는 환경을 고려하여 환경에 맞는 올바른 방법으로 Serialization을 해야 De-serialization이 가능
- Serialization = Memory -> Disk 
- De-serialization = Disk -> Memory



#### Skeleton of handler to serve model



#### Model serving을 위한 다양한 Frameworks

- 딥러닝 모델의 안정적인 serving을 위해 TensorFlow serving나 Torchserve, TensorRT 같은 프레임워크를 사용하는 것이 일반적
- Flask와 같은 웹 프레임워크는 클라이언트로부터의 요청을 처리하기 위해 주로 사용
- 별도의 모델 추론을 위한 API 서버를 운용하여 내부 혹은 외부 통신을 통해 예측/추론값 반환
- 대용량 데이터의 배치처리와 딥러닝 모델의 활용이 늘면서 multi node, multi GPU 환경에서의 안정적인 모델 서빙을 위함



## Inference를 위한 model handler 개발

#### Handle

- 요청 정보를 받아 적절한 응답을 반환

1. 정의된 양식으로 데이터가 입력됐는지 확인
2. 입력 값에 대한 전처리 및 모델에 입력하기 위한 형태로 변환
3. 모델 추론
4. 모델 반환값의 후처리 작업
5. 결과 반환



#### Initialize()

- 데이터 처리나 모델, configuration 등 동기화

1. Configuration 등 동기화
2. (Optional) 신경망을 구성하고 초기화
3. 사전 학습한 모델이나 전처리 불러오기 (De-serialization)

##### Note

- 모델은 전역변수로 불러와야 합니다. 만약 inference를 할 때마다 모델을 불러오도록 한다면 그로 인해 발생하는 시간이나 자원 등의 낭비가 발생합니다.
- 일반적으로 요청을 처리하기 전에 모델을 불러 둡니다.



#### preprocess()

- Raw input을 전처리 및 모델 입력 가능형태로 변환

1. Raw input 전처리
   - 데이터 클린징의 목적과 학습된 모델의 학습 당시 scaling이나 처리 방식과 맞춰주는 것이 필요
2. 모델에 입력가능한 형태로 변환
   - vectorization, converting to id 등의 작업



#### Inference()

- 입력된 값에 대한 예측 / 추론

1. 각 모델의 predict 방식으로 예측 확률분포 값 반환



#### Postprocess()

- 모델의 예측값을 response에 맞게 후처리 작업

1. 예측된 결과에 대한 후처리 작업
2. 보통 모델이 반환하는 건 확률분포와 같은 값이기 때문에 response에서 받아야 하는 정보로 처리하는 역할을 많이 함



## Flask 기반 감정분석 API 개발

- 네이버 영화리뷰 감정분석 개요
- 감정분석 API 개발 방향
- API 정의
- Add Deep Learning model handler
- Unittest
- Flask API 개발 & 배포
- Test API on remote



#### API 정의

key:value 형태의 json 포맷으로 요청을 받아 text indel 별로 ket:value로 결과를 저장한 json 포맷으로 결과 반환

- post 방식으로 predict 요청
- do_fast를 True로 할 경우, 빠른 추론 속도가 가능한 머신러닝 모델로 추론하도록 함
- False의 경우, 추론속도는 비교적 느리지만 정확도가 높은 딮러닝 모델로 추론하도록 함



#### Add Deep Learning model handler

- 사전 학습한 딥러닝 모델을 활용하여 머신러닝 모델 handler와 동일한 입력에 대해 동일한 결과를 반환하는 handler 개발
- 사전 학습한 모델은 Hugging Face에서 제공하는 외부 저장소에서 다운로드 받아 불러옴



#### Flask API 개발 & 배포

- Model을 전역변수로 불러오고 요청된 텍스트에 대해 예측 결과를 반환하는 코드 입력

