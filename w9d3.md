## 심층학습

- 다층 퍼셉트론에 은닉층을 여러 개 추가하면 깊은 신경망이 됨

- 심층학습은 깊은 신경망의 학습

- 심층학습은 새로운 응용을 창출하고 인공지능 제품의 성능을 획기적으로 향상

  -> 현대 기계학습을 주도



#### 심층학습의 등장

- 배경
  - 1980년대에 이미 깊은 신경망 아이디어 등장
  - 하지만 실현 불가능(당시, 깊은 신경망은 학습이 안됨)
  - 경사 소멸 문제
  - 작은 훈련집합
  - 과다한 연산과 시간 소요(낮은 연산의 범용 컴퓨터, 값비싼 슈퍼컴퓨터)
- 일부 연구자들은 실망스러운 상황에서도 지속적인 연구
  - 학습률에 따른 성능 변화 양상
  - 모멘텀과 같은 최적 탐색 방법 모색
  - 은닉 노드 수에 따른 성능 변화
  - 데이터 전처리의 영향
  - 활성함수의 영향
  - 규제 기법의 영향 등



- 경사 소멸 문제
  ![image](https://user-images.githubusercontent.com/60081212/123018810-34786900-d40a-11eb-8b3f-1bdfce60c0bd.png)

-> 깊어질수록 Back-Prop시 vanish 문제가 생길 확률이 높다.



#### 심층학습의 성공 배경

​	![image](https://user-images.githubusercontent.com/60081212/123019281-2a0a9f00-d40b-11eb-916f-2487d938444e.png)

- 기계학습의 새로운 전환
  ![image](https://user-images.githubusercontent.com/60081212/123019618-cb91f080-d40b-11eb-902b-c10882f9fd11.png)

- 고전적인 기계학습과 심층학습 비교 예
  ![image](https://user-images.githubusercontent.com/60081212/123019882-51ae3700-d40c-11eb-8e8f-b0831fc65801.png)

->얕은 은닉층에서 깊은 은닉층으로 갈 수록 추상화 된다.

- 깊은 신경망의 표현학습(또는 특징 학습)
  ![image](https://user-images.githubusercontent.com/60081212/123020171-cd0fe880-d40c-11eb-90da-e0dcfb096096.png)

- 깊은 다층 퍼셉트론의 구조

  ![image](https://user-images.githubusercontent.com/60081212/123020574-735bee00-d40d-11eb-9865-2b14002488c5.png)

- DMLP의 가중치 행렬
  ![image](https://user-images.githubusercontent.com/60081212/123020635-8e2e6280-d40d-11eb-9c16-0ff017138069.png)
- DMLP의 동작
  ![image](https://user-images.githubusercontent.com/60081212/123020661-9be3e800-d40d-11eb-8023-9d5dec6de8e5.png)

- 동작(전방 계산)을 구체적으로 쓰면,
  ![image](https://user-images.githubusercontent.com/60081212/123020739-c0d85b00-d40d-11eb-8fee-c8165bdc217a.png)

- DMLP 학습은 기존 MLP 학습과 유사
  - DMLP는 경사도 계산과 가중치 갱신을 더 많은 단계(층)에 걸쳐 수행
- 오류 역전파 알고리즘
  ![image](https://user-images.githubusercontent.com/60081212/123020960-29bfd300-d40e-11eb-9ee5-b70d8566e5ae.png)

![image](https://user-images.githubusercontent.com/60081212/123022611-fdf21c80-d410-11eb-99c2-9afafb7cbbea.png)

- 역사적 고찰
  ![image](https://user-images.githubusercontent.com/60081212/123022765-3bef4080-d411-11eb-949e-897dbac9cf43.png)

- 종단간(end-to-end) 최적화된 학습 가능
  ![image](https://user-images.githubusercontent.com/60081212/123022959-88d31700-d411-11eb-9ba7-46d05c9834a9.png)

- 깊이의 중요성
  - 점선은 20개 노드를 가진 은닉층 하나 짜리 신경망
  - 실선은 각각 10개 노드를 가진 은닉층 두 개 짜리 신경망 -> 더 정교한 분할
    ![image](https://user-images.githubusercontent.com/60081212/123023129-d8194780-d411-11eb-81b1-3df9d5f2d459.png)
- 계층적 특징
  ![image](https://user-images.githubusercontent.com/60081212/123023281-19115c00-d412-11eb-8259-2f05d037daf1.png)

![image](https://user-images.githubusercontent.com/60081212/123023416-4bbb5480-d412-11eb-980a-a639d3b94904.png)

## 신경망의 기초-실습

![image](https://user-images.githubusercontent.com/60081212/123023737-def48a00-d412-11eb-9327-ce528e811cbb.png)

![image](https://user-images.githubusercontent.com/60081212/123023796-ffbcdf80-d412-11eb-80db-e0ceffb174a0.png)

![image](https://user-images.githubusercontent.com/60081212/123023906-3561c880-d413-11eb-93bf-2831466f9328.png)

![image](https://user-images.githubusercontent.com/60081212/123024113-84a7f900-d413-11eb-8f33-5885f8d95d22.png)

![image](https://user-images.githubusercontent.com/60081212/123024751-8920e180-d414-11eb-8fa4-d4768e3c4084.png)

