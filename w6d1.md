## Machin Learning

Machine Learning(기계학습)

- 경험을 통해 자동으로 개선하는 컴퓨터 알고리즘의 연구
- 학습데이터: 입력벡터들 X<sub>1</sub> ... X<sub>N</sub>, 목표값들  T<sub>1</sub> ... T<sub>N</sub>
- 머신러닝 알고리즘의 결과는 목표값을 예측하는 함수 y(x)

##### ex: 숫자 인식



#### 핵심개념들

- 학습단계(training or learning phase): 함수 y(x)를 학습데이터에 기반해 결정하는 단계
- 시험셋(test set): 모델을 평가하기 위해서 사용하는 새로운 데이터
- 일반화(generalizetion): 모델에서 학습에 사용된 데이터가 아닌 이전에 접하지 못한 새로운 데이터에 대해 올바른 예측을 수행하는 역량
- 지도학습(supervised learning): target이 주어진 경우
  - 분류(classification)
  - 회귀(regression)
- 비지도학습(unsupervised learning): target이 없는 경우
  - 군집(clustering)



##### 다항식 곡선 근사(polynomial Curve Fitting)

- 학습데이터 : 입력벡터 **X** = (x<sub>1</sub>, ... x<sub>N</sub>)<sup>T</sup>, **t** = (t<sub>1</sub>, ... t<sub>N</sub>)<sup>T</sup>

- 목표 : 새로운 입력벡터 x가 주어졌을 때 목표값 t를 예측하는 것

- 확률이론(probability theory): 예측값의 불확실성을 정량화시켜 표현할 수 있는 수학적인 프레임워크를 제공한다.

- 결정이론(decision theory): 확률적 표현을 바탕으로 최적의 예측을 수행할 수 있는 방법론을 제공한다.

- A polynomial function linear in w

  - y(x, **w**) = w<sub>0</sub> + w<sub>1</sub>x + ... w<sub>M</sub>x<sup>M</sup>
    - x = 데이터, w = 고정되어 있지만 알지 못하는 값
    - M은 알아내야 할 미지수의 갯수일 것(?)

- ##### 오차함수(Error Function)

  - (오차)<sup>2</sup>의 합 / 2

- 과소적합(Under-fitting)과 과대적합(Over-fitting)

  - 과소적합은 데이터에 대해 변화가 너무 적을 때
  - 과대적합은 데이터마다 변화가 너무 클 때

- 데이터가 충분히 많으면 M이 커도 원하는 결과값을 얻을 수 있다.

- 규제화(Regularization)

  - w를 작게 만들어 오차를 잡는 방법 λ가 커질수록 w를 작게 만들어 영향을 줄인다.



## 확률이론(probability Theory)

##### 확률변수(Random Variable)

확률변수 X는 표본의 집합 S의 원소 e를 실수값 X(e) = x에 대응시키는 함수이다.

- ex: 동전을 던져서 앞면이 나올 확률 **S**
- 대문자 X, Y, ... : 확률변수
- 소문자 x, y, ...: 확률변수가 가질 수 있는 값
- 확률 P는 집합 S의 부분집합을 실수값에 대응시키는 함수
- P[X = x]
- P[x <= x]
- X = x, X <= x는 집합 S의 부분집합을 정의한다.
- 확률변수는 함수로 표현됨을 잊지 말자.

##### 연속확률변수(Continuous Random Variables)

- 누적분포함수(cumulative distribution function, CDF): F(x) = P[X **∈** (-**∞**, x)]
- 누적분포함수 F(x)를 가진 확률변수 X에 대해서 다음을 만족하는 함수 f(x)가 존재한다면 X를 연속확률변수라고 부르고 f(x)를 X의 확률밀도함수(probability density function, pdf)라고 부른다.
- 확률변수를 명확히 하기 위해 F<sub>X</sub>(x)로 쓰기도 한다.
- 혼란이 없을 경우 f<sub>X</sub>(x)대신 p<sub>X</sub>(x)를 사용하기도 한다.
- p(x) >= 0, p(x)의 면적의 합(p의 모든 확률) = 1

##### 확률변수의  성질(The Rules of Probability)

- 덧셈법칙(sum rule)
  - X, Y가 주어졌을 때, Y에 대해서 합을 구하고 적분해 X를 구할 수 있다.
- 곱셈법칙(product rule)
  - X, Y의 확률은 Y가 주어졌을 때, X의 확률과 Y의 확률의 곱 혹은 X가 주어졌을 때 Y의  확률과 X의 확률의 곱으로 구할 수 있다.
- 베이즈 확률(Baeys)
  - X, Y의 확률 중에서 Y가 주어졌을 때의 X의 확률보다 X가 주어졌을 때 Y의 확률이 구하는 것이 용이할 때 사용
  - 사후확률(posterior) = (가능도(우도)(likelihood) x 사전확률(prior)) / (normalization)Y와 상관없는 상수, X의 경계확률(marginal) p(X)
- 예제 주변확률(Marginal)분포, 조건부(Conditional)확률 분포

##### 확률변수의 함수(Functions of Random Variables)

- 확률변수 X의 함수 Y = f(X)도 확률변수이다.(함수의 함수). 예를 들어 확률변수 X가 주(week)의 수로 표현되었다라고 할 때 일(day)의 수로 표현된 새로운 확률변수를 정의할 수 있다.
- Y = 7X
- P[14 <= Y <= 21] = P[2 <= X <= 3]
- 확률변수 X의 함수 Y = g(X)와 역함수 w(Y) = X 가 주어졌을 때 다음이 성립한다.
- p<sub>y</sub>(y) = p<sub>x</sub>|dx/dy|
- k차원의 확률변수 벡터 x가 주어졌을 때, k개의 x에 관함 함수들 y<sub>i</sub>는 새로운 확률변수벡터 y = (y<sub>1</sub>, ... y<sub>n</sub>)을 정의한다. 
  - y<sub>1</sub> = g<sub>1</sub>(x<sub>1</sub>, ... x<sub>k</sub>)
- 간략하게 y = g(**x**)로 나타낼 수 있다. 만약 y = g(**x**)가 일대일변환인 경우(x = w(y)로 유일한 해를 가질 때,) y의 결합확률밀도함수(joing pdf)는 자코비안 행렬로 표현할 수 있다.
- ex:
  - **p**<sub>x<sub>1</sub></sub>, <sub>x<sub>2</sub></sub>(x<sub>1</sub>, x<sub>2</sub>) = e<sup>-(x<sub>1</sub> + x<sub>2</sub>)</sup>, x<sub>1</sub> > 0, x<sub>2</sub> > 0 라고 하자.
  - y<sub>1</sub> = x<sub>1</sub>, y<sub>2</sub> = x<sub>1</sub> + x<sub>2</sub>에 의해서 정의되는 y의 pdf는?
  - x<sub>1</sub> = y<sub>1</sub>
  - x<sub>2</sub> = y<sub>2</sub> - x<sub>1</sub> = y<sub>2</sub> -  y<sub>1</sub>
  - J = [dx<sub>1</sub>/dy<sub>1</sub>  dx<sub>1</sub>/dy<sub>2</sub>
  - ​      dx<sub>2</sub>/dy<sub>1</sub>  dx<sub>2</sub>/dy<sub>2</sub>]
  - = [1  0
  - ​    -1  1]
  - f<sub>y<sub>1</sub>, y<sub>2</sub></sub>(y<sub>1</sub>, y<sub>2</sub>) = f<sub>x<sub>1</sub>, x<sub>2</sub></sub>(x<sub>1</sub>, x<sub>2</sub>) [ J ] = f<sub>x<sub>1</sub>, x<sub>2</sub></sub>(y<sub>1</sub>, y<sub>2</sub> - y<sub>1</sub>) = e<sup>-{y<sub>1</sub> + (y<sub>2</sub> - y<sub>1</sub>)}</sup> = e<sup>-y<sub>2</sub></sup>

##### Inverse CDF Technique

- 확률변수 X가 CDF F<sub>x</sub>(x)를 가진다고 하자. 연속확률분표함수 U ~ UNIF(0,1)의 함수로 정의되는 다음 확률변수 Y를 생각해보자
- Y = F<sub>x</sub><sup>-1</sup>(U)
- 확률변수 Y는 확률변수 X와 동일한 분포를 따르게 된다.

- 반경이 r인 원 안에 랜덤하게 점들을 찍는 프로그램?
  - d(점과 중심사이의 거리)인 원의 넓이 / r인 원의 넓이
  - f = d<sup>2</sup> / r<sup>2</sup>
  - f<sup>-1</sup> = d = r(u)<sup>1/2</sup>
  - r(u)<sup>1/2</sup> 가 아닌 ru 로 프로그램을 짤 경우, 중심에 분포가 집중되게 된다.

##### 기댓값(Expectations)

- 기댓값: 확률분포 p(x)하에서 함수 f(x)의 평균값
- 이산확률분포(discrete distribution)
- 연속확률분포(continous distrbution)
- 확률분포로부터 N개의 샘플을 추출해서 기댓값을 근사할 수 있다.

##### 분산(variance), 공분산(covariance)

- f(x)의 분산(variance): f(x)의 값들이 기댓값으로부터 흩어져 있는 정도

##### 빈도주의 대 베이지안(Frequentist versus Bayesian)

- 확률을 해석하는 두 가지 다른 관점: 빈도주의, 베이지안
  - 빈도주의: 반복가능한 사건들의 빈도수에 기반
  - 베이지안: 불확실성을 정량적으로 표현
- 반복가능하지 않은 사건일 경우: 북극얼음이 이번 세기말까지 녹아 없어질 확률? 우리가 이미 알고 있는 정보(얼음이 녹고 있는 속도)에 근거해 확률을 정량적으로 나타낼 수 있고 새로 수집하는 정보에 따라 확률을 업데이트할 수 있다.
- 모델의 파라미터 **w**(예를 들어 다항식 곡선 근사문제에서의 계수 w)에 대한 우리의 지식을 확률적으로 나타내고 싶다면?
  - w에 대한 사전지식 p(w) => 사전확률
  - 새로운 데이터  D = {t1, ... tn}를 관찰하고 난 뒤의 조건부확률 p(D|w) => 우도함수
  - 특정 w값에 대해 D의 관찰값이 얼마나 가능성이 있는지를 나타냄.  w에 관한 함수임을 기억할 것.
    - p(w|D) = (p(D|w)p(w)) / p(D)
  - p(w|D)는 D를 관찰하고 난 뒤의 w에 대한 불확실성을 표현
- 반면, 빈도주의는 **w**가 고정된 파라미터이고 최대우도와 같은 '추정자(estimator)'를 사용해서 그 값을 구한다. 구해진 파라미터의 불확실성은 부트스트랩(bootstrap)방법을 써서 구할 수 있다.
- 베이지안 관점의 장점
  - 사전확률을 모델에 포함시킬 수 있다.
    - 동전을 던져서 세 번다 앞 면이 나왔을 때
      - 최대우도: 앞 면이 나올 확률은 1이 됨
      - 베이지안: 극단적인 확률을 피할 수 있음(사전 정보 반영)



## 정규분포(Gaussian Distribution)

- 단일변수 x를 위한 가우시안 분포
  - N(x|μ,σ2)=1(2πσ2)1/2exp{−12σ2(x−μ)2}
- 가우시안 분포가 정규화됨(normalized)을 보일 것임.
- 분산 var[x] = y = σ<sup>2</sup>
- 최대우도해(Maximum Likelihood solution)
  - **X** = (x1, ..., xn)<sup>T</sup>가 독립적으로 같은 가우시안분포로부터 추출된 N개의 샘플들이라고 할 때,
  - μ<sub>ML</sub> = 1부터N까지 X<sub>n</sub> 의 합 / N
  - y = σ<sup>2</sup>일 때, y<sub>ML</sub> = σ<sup>2</sup><sub>ML</sub> = 1부터 N까지 (X<sub>n</sub> - μ<sub>ML</sub>)<sup>2</sup> 의 합 / N



## 곡선근사(Curve Fitting): 확률적 관점

- 학습데이터 : 입력벡터 **X** = (x<sub>1</sub>, ... x<sub>N</sub>)<sup>T</sup>, **t** = (t<sub>1</sub>, ... t<sub>N</sub>)<sup>T</sup>
- 목표값 **t**의 불확실성을 다음과 같이 확률분포로 나타낸다.
  - p(t|x, **w**, β) = Ν(t|y(x, **w**), β<sup>-1</sup>)
- 파라미터: **w**, β
- 파라미터들의 최대우도해를 구해보자.
- 우도함수
- 로그우도함수
- w에 관해서 우도함수를 최대화시키는 것은 제곱합 오차함수를 최소화시키는 것과 동일

##### 다음 단계 : 사전확률 포함

- 파라미터 w의 사전확률을 가정했을 때, w의 사후확률은 우도함수와 사전확률의 곱에 비례한다.
- 이 사후확률을 최대화시키는 것은 규제화된 제곱합 오차함수를 최소화시키는 것과 동일하다.

##### 최종단계 : 완전한 베이지안 곡선근사

- 이제까지 t의 예측분포를 구하기 위해 여전히 **w**의 점추정에 의존해왔다. 완전한 베이지안 방법은 w의 분포로부터 확률의 기본법칙만을 사용해서 t의 예측분포를 유도한다.
- 이 예측 분포도 가우시안분포라는 사실과 그 분포의 평균벡터와 공분산행렬을 구하는 방법을 다음에 알아볼 것이다.

